{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1G_deXmn7abxcqfPfwxSJNplgV81ykjxa","timestamp":1750956113791},{"file_id":"1wowI30TVgwhpayB-8bqRE63XyGK8--_k","timestamp":1750886297563},{"file_id":"1dCJMxO_oQvO06mlzjwco2Uu9R24KFAPA","timestamp":1750798903093}],"gpuType":"A100","authorship_tag":"ABX9TyMsUK9TM9E6UeBt2ITiuJV1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HB0jnU92N8OR","executionInfo":{"status":"ok","timestamp":1751311324365,"user_tz":240,"elapsed":632,"user":{"displayName":"opeyemi adeniran","userId":"06352095503427961436"}},"outputId":"7c4f7f8f-1bdb-4816-a9a2-eef071a961f7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["pip install anthropic pandas"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uousQAcov9uo","executionInfo":{"status":"ok","timestamp":1751311330982,"user_tz":240,"elapsed":6620,"user":{"displayName":"opeyemi adeniran","userId":"06352095503427961436"}},"outputId":"0c5e1728-0fed-49b9-f0ab-7fb76d9ec276"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting anthropic\n","  Downloading anthropic-0.55.0-py3-none-any.whl.metadata (27 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.9.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.28.1)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.10.0)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (2.11.7)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.14.0)\n","Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (2025.6.15)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n","Downloading anthropic-0.55.0-py3-none-any.whl (289 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m289.3/289.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: anthropic\n","Successfully installed anthropic-0.55.0\n"]}]},{"cell_type":"markdown","source":["# CLAUDE"],"metadata":{"id":"0SuW6Bh-tbRo"}},{"cell_type":"code","source":["\"\"\"\n","GROUND TRUTH Forensic Analysis Performance Evaluator using Claude Sonnet 4\n","Requirements: pip install anthropic pandas\n","\n","üöÄ GROUND TRUTH EVALUATION MODE: Evaluates ground truth forensic analyses!\n","\n","‚úÖ MODIFIED VERSION: Now evaluates ground truth data instead of technique comparisons!\n","üîí GUARANTEED 100% DATA PROCESSING: Verifies all ground truth files are processed!\n","ü§ñ CLAUDE SONNET 4: Uses latest Claude Sonnet 4 (claude-sonnet-4-20250514) model!\n","\n","Features:\n","- ‚úÖ Processes ground truth evaluations in optimized batches\n","- ‚úÖ Concurrent execution with thread pool for speed\n","- ‚úÖ Batch-level checkpointing and error recovery\n","- ‚úÖ Smart load balancing across API calls\n","- ‚úÖ Automatic retry logic for failed evaluations\n","- ‚úÖ Progress tracking at both batch and individual level\n","- ‚úÖ Memory-optimized data loading per batch\n","- ‚úÖ Claude Sonnet 4 with enhanced context window (300K+ chars)\n","\n","‚ö° PERFORMANCE BENEFITS:\n","- 6x faster processing through parallelization\n","- Better API utilization with concurrent requests\n","- Reduced total evaluation time from hours to minutes\n","- Intelligent batch sizing to respect rate limits\n","- Enhanced capabilities with Claude Sonnet 4\n","\n","üéØ GROUND TRUTH STRATEGY: Evaluates each ground truth analysis against forensic standards\n","and provides comprehensive quality assessment for reference data.\n","\n","üîß MODIFICATION: Adapted for ground truth evaluation with flat file structure!\n","ü§ñ MODEL: Uses Claude Sonnet 4 (claude-sonnet-4-20250514) as primary model!\n","\"\"\"\n","\n","import json\n","import os\n","import pandas as pd\n","from pathlib import Path\n","import anthropic\n","import time\n","from datetime import datetime\n","import re\n","import concurrent.futures\n","import threading\n","from collections import defaultdict\n","import logging\n","\n","# Configuration - UPDATED FOR GROUND TRUTH\n","API_KEY_FILE = \"/content/drive/Shareddrives/DR KOFI RESEARCH/RESEARCH/PROMPTS/API-KEYS/claude.txt\"\n","SOURCE_DIR = \"/content/drive/Shareddrives/DR KOFI RESEARCH/RESEARCH/PROMPTS/GroundTruth\"  # Updated path\n","OUTPUT_DIR = \"/content/drive/Shareddrives/DR KOFI RESEARCH/RESEARCH/PROMPTS/GROUNDTRUTH-EVALUATION-RESULTS\"  # Updated output\n","\n","# Batch processing configuration\n","BATCH_SIZE = 6  # Number of evaluations per batch\n","MAX_WORKERS = 6  # Number of concurrent threads\n","RETRY_ATTEMPTS = 10  # Attempts per individual evaluation\n","BATCH_RETRY_ATTEMPTS = 3  # Retry failed batches\n","INTER_BATCH_DELAY = 10  # Seconds between batches\n","\n","# Thread-safe counters\n","evaluation_lock = threading.Lock()\n","success_counter = 0\n","failure_counter = 0\n","truncation_counter = 0\n","\n","def setup_logging(output_dir):\n","    \"\"\"Set up logging for batch processing\"\"\"\n","    log_dir = os.path.join(output_dir, \"logs\")\n","    os.makedirs(log_dir, exist_ok=True)\n","\n","    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","    log_file = os.path.join(log_dir, f\"groundtruth_evaluation_{timestamp}.log\")\n","\n","    logging.basicConfig(\n","        level=logging.INFO,\n","        format='%(asctime)s - %(levelname)s - %(message)s',\n","        handlers=[\n","            logging.FileHandler(log_file),\n","            logging.StreamHandler()\n","        ]\n","    )\n","\n","    return log_file\n","\n","# Read API key from file\n","def load_api_key():\n","    try:\n","        with open(API_KEY_FILE, \"r\") as f:\n","            api_key = f.read().strip()\n","        print(\"‚úÖ Claude API key loaded successfully\")\n","        logging.info(\"‚úÖ Claude API key loaded successfully\")\n","        return api_key\n","    except Exception as e:\n","        print(f\"‚ùå Error loading API key: {e}\")\n","        logging.error(f\"‚ùå Error loading API key: {e}\")\n","        return None\n","\n","# Initialize Anthropic client\n","ANTHROPIC_API_KEY = load_api_key()\n","if ANTHROPIC_API_KEY:\n","    try:\n","        client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n","        print(\"‚úÖ Claude client initialized\")\n","        logging.info(\"‚úÖ Claude client initialized\")\n","\n","        # Test API connection\n","        print(\"üîç Testing API connection...\")\n","        logging.info(\"üîç Testing API connection...\")\n","        test_response = client.messages.create(\n","            model=\"claude-sonnet-4-20250514\",\n","            max_tokens=10,\n","            messages=[{\"role\": \"user\", \"content\": \"Hi\"}]\n","        )\n","        print(\"‚úÖ API connection successful\")\n","        logging.info(\"‚úÖ API connection successful\")\n","    except Exception as e:\n","        print(f\"‚ö†Ô∏è  API setup warning: {e}\")\n","        logging.warning(f\"‚ö†Ô∏è  API setup warning: {e}\")\n","        print(\"üîÑ Continuing anyway - will test during evaluation\")\n","        client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY) if ANTHROPIC_API_KEY else None\n","else:\n","    client = None\n","    print(\"‚ùå Failed to initialize Claude client\")\n","    logging.error(\"‚ùå Failed to initialize Claude client\")\n","\n","# Evaluation criteria - UPDATED FOR GROUND TRUTH EVALUATION\n","EVALUATION_CRITERIA = \"\"\"\n","GROUND TRUTH EVALUATION CRITERIA:\n","Evaluate how well this ground truth analysis serves as a reference standard for forensic analysis training and evaluation.\n","\n","1. Crime Classification and Intent Detection (10 points)\n","Forensic Question: How accurately and comprehensively does the ground truth identify and classify all criminal offenses with precise legal terminology?\n","Rating Scale:\n","10 points: Perfect identification of all crime types with expert-level legal precision, suitable as definitive reference\n","8-9 points: Highly accurate crime classification with minor gaps, excellent reference quality\n","6-7 points: Good crime identification but missing some nuances or secondary offenses\n","3-5 points: Basic crime classification with significant gaps or inaccuracies\n","0-2 points: Poor crime identification, unsuitable as reference standard\n","\n","2. Temporal Forensic Reconstruction (10 points)\n","Forensic Question: How precisely does the ground truth establish the complete chronological sequence with forensic-quality timestamps?\n","Rating Scale:\n","10 points: Exemplary timeline reconstruction with precise timestamps, perfect reference standard\n","8-9 points: Excellent chronology with most critical timestamps, high-quality reference\n","6-7 points: Good temporal reconstruction but missing some key markers\n","3-5 points: Basic timeline with significant gaps\n","0-2 points: Poor temporal documentation, inadequate for reference use\n","\n","3. Subject Identification and Behavioral Analysis (10 points)\n","Forensic Question: How thoroughly does the ground truth document all identifying features and behavioral patterns for training purposes?\n","Rating Scale:\n","10 points: Comprehensive subject documentation with expert-level behavioral analysis, ideal training reference\n","8-9 points: Detailed subject profiles with strong behavioral insights, excellent reference quality\n","6-7 points: Good identification details but lacks some forensic descriptors\n","3-5 points: Basic subject documentation with limited behavioral analysis\n","0-2 points: Inadequate subject identification for training purposes\n","\n","4. Physical Evidence Documentation (10 points)\n","Forensic Question: How completely does the ground truth catalog all physical evidence with proper forensic methodology?\n","Rating Scale:\n","10 points: Complete evidence inventory with expert forensic documentation, perfect reference standard\n","8-9 points: Comprehensive evidence documentation with strong forensic methodology\n","6-7 points: Good evidence tracking but missing some items or details\n","3-5 points: Basic evidence documentation with gaps\n","0-2 points: Poor evidence documentation, unsuitable for reference\n","\n","5. Violence Assessment and Weapon Analysis (10 points)\n","Forensic Question: How expertly does the ground truth document violence dynamics, weapons, and force patterns?\n","Rating Scale:\n","10 points: Expert-level violence and weapon analysis, definitive reference quality\n","8-9 points: Excellent violence documentation with detailed weapon analysis\n","6-7 points: Good violence assessment but missing some analytical depth\n","3-5 points: Basic violence documentation with limited analysis\n","0-2 points: Poor violence assessment, inadequate for reference use\n","\n","6. Criminal Network and Coordination Analysis (10 points)\n","Forensic Question: How completely does the ground truth map criminal relationships and communication patterns?\n","Rating Scale:\n","10 points: Complete criminal network mapping with expert analysis, perfect reference\n","8-9 points: Excellent network analysis with detailed relationship documentation\n","6-7 points: Good coordination analysis but missing some connections\n","3-5 points: Basic network documentation with gaps\n","0-2 points: Poor network analysis, unsuitable for reference\n","\n","7. Modus Operandi Documentation (10 points)\n","Forensic Question: How expertly does the ground truth identify and document criminal methods for pattern recognition training?\n","Rating Scale:\n","10 points: Expert MO analysis with comprehensive signature behavior documentation, ideal reference\n","8-9 points: Excellent MO identification with detailed criminal methodology\n","6-7 points: Good MO documentation but lacks some analytical depth\n","3-5 points: Basic MO identification with limited detail\n","0-2 points: Poor MO analysis, inadequate for training purposes\n","\n","8. Scene Analysis and Environmental Context (10 points)\n","Forensic Question: How thoroughly does the ground truth document scene characteristics and environmental factors?\n","Rating Scale:\n","10 points: Complete scene analysis with expert environmental assessment, perfect reference\n","8-9 points: Excellent scene documentation with comprehensive context analysis\n","6-7 points: Good scene analysis but missing some environmental factors\n","3-5 points: Basic scene documentation with limited context\n","0-2 points: Poor scene analysis, unsuitable for reference use\n","\n","9. Escape Route and Exit Strategy Analysis (10 points)\n","Forensic Question: How completely does the ground truth reconstruct escape routes and document exit strategies?\n","Rating Scale:\n","10 points: Complete escape analysis with expert route mapping, definitive reference quality\n","8-9 points: Excellent escape documentation with detailed route analysis\n","6-7 points: Good escape route identification but missing some details\n","3-5 points: Basic escape documentation with gaps\n","0-2 points: Poor escape analysis, inadequate for reference\n","\n","10. Reference Quality and Training Utility (10 points)\n","Forensic Question: How well does the ground truth serve as a comprehensive reference standard for training and evaluation?\n","Rating Scale:\n","10 points: Exemplary reference quality with perfect structure, ideal for training and evaluation\n","8-9 points: Excellent reference standard with high training utility\n","6-7 points: Good reference quality but could be enhanced for training\n","3-5 points: Basic reference quality with limited training utility\n","0-2 points: Poor reference quality, unsuitable for training or evaluation purposes\n","\"\"\"\n","\n","def load_groundtruth_files(directory):\n","    \"\"\"Load all ground truth JSON files from the directory - MODIFIED FOR GROUND TRUTH STRUCTURE\"\"\"\n","    print(f\"üîç Scanning ground truth directory: {directory}\")\n","    logging.info(f\"üîç Scanning ground truth directory: {directory}\")\n","\n","    groundtruth_data = {}\n","    stats = {'total_files': 0, 'loaded_files': 0, 'failed_files': 0}\n","\n","    if not os.path.exists(directory):\n","        print(f\"‚ùå Directory not found: {directory}\")\n","        logging.error(f\"‚ùå Directory not found: {directory}\")\n","        return groundtruth_data, stats\n","\n","    # Get all ground truth JSON files\n","    json_files = [f for f in os.listdir(directory)\n","                  if f.endswith('-GroundTruth.json') or f.endswith('GroundTruth.json')]\n","\n","    print(f\"üìÅ Found {len(json_files)} ground truth files\")\n","    logging.info(f\"üìÅ Found {len(json_files)} ground truth files\")\n","\n","    for json_file in json_files:\n","        stats['total_files'] += 1\n","\n","        # Extract crime category from filename (e.g., \"Abuse-GroundTruth.json\" -> \"Abuse\")\n","        if '-GroundTruth.json' in json_file:\n","            crime_category = json_file.replace('-GroundTruth.json', '')\n","        else:\n","            crime_category = json_file.replace('GroundTruth.json', '').rstrip('-')\n","\n","        file_path = os.path.join(directory, json_file)\n","\n","        try:\n","            with open(file_path, 'r', encoding='utf-8') as f:\n","                data = json.load(f)\n","\n","                # Validate data is not empty\n","                if data and (not isinstance(data, dict) or len(data) > 0):\n","                    groundtruth_data[crime_category] = data\n","                    stats['loaded_files'] += 1\n","                    print(f\"    ‚úÖ Loaded: {crime_category}\")\n","                    logging.info(f\"    ‚úÖ Loaded: {crime_category}\")\n","                else:\n","                    print(f\"    ‚ö†Ô∏è  Empty file: {crime_category}\")\n","                    logging.warning(f\"    ‚ö†Ô∏è  Empty file: {crime_category}\")\n","                    stats['failed_files'] += 1\n","        except Exception as e:\n","            print(f\"    ‚ùå Error loading {json_file}: {e}\")\n","            logging.error(f\"    ‚ùå Error loading {json_file}: {e}\")\n","            stats['failed_files'] += 1\n","\n","    print(f\"\\nüìä Ground Truth Loading Summary:\")\n","    print(f\"  Total files found: {stats['total_files']}\")\n","    print(f\"  Successfully loaded: {stats['loaded_files']}\")\n","    print(f\"  Failed to load: {stats['failed_files']}\")\n","    logging.info(f\"üìä Ground Truth Loading Summary: {stats['loaded_files']}/{stats['total_files']} loaded successfully\")\n","\n","    return groundtruth_data, stats\n","\n","def truncate_analysis_data(analysis_data, max_chars=300000):\n","    \"\"\"Truncate analysis data to fit within token limits (Claude Sonnet 4 has even larger context window)\"\"\"\n","\n","    if isinstance(analysis_data, dict):\n","        combined_text = \"\"\n","        total_chars = 0\n","\n","        for key, value in analysis_data.items():\n","            if isinstance(value, dict):\n","                section = f\"\\n--- Ground Truth Analysis {key} ---\\n\" + json.dumps(value, indent=2)\n","            else:\n","                section = f\"\\n--- Ground Truth Analysis {key} ---\\n{str(value)}\\n\"\n","\n","            # Check if adding this section would exceed limit\n","            if total_chars + len(section) > max_chars:\n","                # Add truncation notice and break\n","                combined_text += f\"\\n\\n[NOTE: Ground truth analysis truncated due to length. Showing first {total_chars:,} characters of {total_chars + len(str(analysis_data)) - len(combined_text):,} total characters]\"\n","                break\n","\n","            combined_text += section\n","            total_chars += len(section)\n","\n","        return combined_text\n","    else:\n","        text = str(analysis_data)\n","        if len(text) > max_chars:\n","            return text[:max_chars] + f\"\\n\\n[NOTE: Ground truth analysis truncated. Showing first {max_chars:,} characters of {len(text):,} total characters]\"\n","        return text\n","\n","def create_groundtruth_evaluation_prompt(crime_category, groundtruth_data):\n","    \"\"\"Create evaluation prompt for ground truth analysis - MODIFIED FOR GROUND TRUTH EVALUATION\"\"\"\n","\n","    # Truncate data to prevent token limit issues (Claude Sonnet 4 has very large context window)\n","    combined_text = truncate_analysis_data(groundtruth_data, max_chars=300000)\n","\n","    prompt = f\"\"\"\n","You are a forensic analysis expert evaluating GROUND TRUTH reference data.\n","\n","EVALUATION TASK:\n","Evaluate the following GROUND TRUTH forensic analysis for a {crime_category} incident. This is reference-quality data that should serve as a training standard for forensic analysts.\n","\n","EVALUATION CRITERIA:\n","{EVALUATION_CRITERIA}\n","\n","GROUND TRUTH ANALYSIS TO EVALUATE:\n","{combined_text}\n","\n","EVALUATION INSTRUCTIONS:\n","1. Carefully read the provided ground truth analysis\n","2. Score each of the 10 criteria on a scale of 0-10 points\n","3. Assess whether this analysis meets the standards expected of reference-quality ground truth data\n","4. Provide your evaluation in the following EXACT format:\n","\n","SCORES:\n","1. Crime Classification and Intent Detection: [score]/10\n","2. Temporal Forensic Reconstruction: [score]/10\n","3. Subject Identification and Behavioral Analysis: [score]/10\n","4. Physical Evidence Documentation: [score]/10\n","5. Violence Assessment and Weapon Analysis: [score]/10\n","6. Criminal Network and Coordination Analysis: [score]/10\n","7. Modus Operandi Documentation: [score]/10\n","8. Scene Analysis and Environmental Context: [score]/10\n","9. Escape Route and Exit Strategy Analysis: [score]/10\n","10. Reference Quality and Training Utility: [score]/10\n","\n","TOTAL SCORE: [sum]/100\n","\n","BRIEF JUSTIFICATION:\n","[Provide 2-3 sentences explaining the overall assessment of this ground truth analysis quality]\n","\n","Be objective and rigorous in your scoring. Ground truth data should meet the highest forensic standards and serve as excellent reference material for training and evaluation.\n","\"\"\"\n","\n","    return prompt\n","\n","def evaluate_with_claude(prompt, evaluation_id, max_retries=RETRY_ATTEMPTS):\n","    \"\"\"Send evaluation prompt to Claude API - optimized for batch processing\"\"\"\n","\n","    for attempt in range(max_retries):\n","        try:\n","            print(f\"    ü§ñ {evaluation_id} - Attempt {attempt + 1}\")\n","            logging.debug(f\"    ü§ñ {evaluation_id} - Attempt {attempt + 1}\")\n","\n","            # Progressive truncation for batch efficiency\n","            current_prompt = prompt\n","            if attempt >= 2:\n","                # More generous limits for Claude Sonnet 4\n","                base_chars = 250000 if \"sonnet-4\" in str(models_to_try[0]) else 200000\n","                reduction_per_attempt = 50000  # Less aggressive reduction for larger context\n","                max_chars = max(30000, base_chars - (attempt * reduction_per_attempt))\n","\n","                lines = prompt.split('\\n')\n","                analysis_start = None\n","                for i, line in enumerate(lines):\n","                    if \"GROUND TRUTH ANALYSIS TO EVALUATE:\" in line:\n","                        analysis_start = i + 1\n","                        break\n","\n","                if analysis_start:\n","                    pre_analysis = '\\n'.join(lines[:analysis_start])\n","                    analysis_section = '\\n'.join(lines[analysis_start:])\n","\n","                    if len(analysis_section) > max_chars:\n","                        truncated = analysis_section[:max_chars]\n","                        analysis_section = truncated + f\"\\n\\n[BATCH TRUNCATION: Attempt {attempt + 1}, {max_chars:,} chars]\"\n","\n","                    current_prompt = pre_analysis + analysis_section\n","\n","            # Model selection optimized for batch processing\n","            models_to_try = [\n","                \"claude-sonnet-4-20250514\",      # Latest Claude Sonnet 4 - most capable\n","                \"claude-3-5-sonnet-20241022\",    # Fallback to Claude 3.5 Sonnet\n","                \"claude-3-haiku-20240307\"        # Fast model for batch processing\n","            ]\n","\n","            for model in models_to_try:\n","                try:\n","                    # Optimized settings for each model\n","                    if \"sonnet-4\" in model:\n","                        max_tokens = 2500  # Claude Sonnet 4 - highest capacity\n","                    elif \"3-5-sonnet\" in model:\n","                        max_tokens = 2000  # Claude 3.5 Sonnet\n","                    elif \"haiku\" in model:\n","                        max_tokens = 1500  # Claude 3 Haiku - faster processing\n","                    else:\n","                        max_tokens = 2000  # Default\n","\n","                    response = client.messages.create(\n","                        model=model,\n","                        max_tokens=max_tokens,\n","                        temperature=0.1,\n","                        messages=[{\n","                            \"role\": \"user\",\n","                            \"content\": current_prompt\n","                        }]\n","                    )\n","\n","                    print(f\"    ‚úÖ {evaluation_id} - SUCCESS with {model}\")\n","                    logging.info(f\"    ‚úÖ {evaluation_id} - SUCCESS with {model}\")\n","                    return response.content[0].text\n","\n","                except Exception as model_error:\n","                    print(f\"    ‚ùå {evaluation_id} - {model} failed: {str(model_error)[:50]}...\")\n","                    logging.warning(f\"    ‚ùå {evaluation_id} - {model} failed: {str(model_error)[:50]}...\")\n","                    continue\n","\n","        except Exception as e:\n","            print(f\"    ‚ùå {evaluation_id} - API Error (attempt {attempt + 1}): {str(e)[:100]}\")\n","            logging.error(f\"    ‚ùå {evaluation_id} - API Error (attempt {attempt + 1}): {str(e)[:100]}\")\n","\n","            error_str = str(e).lower()\n","\n","            if \"too long\" in error_str or \"prompt is too long\" in error_str:\n","                # Emergency truncation for batch processing\n","                if \"GROUND TRUTH ANALYSIS TO EVALUATE:\" in current_prompt:\n","                    parts = current_prompt.split(\"GROUND TRUTH ANALYSIS TO EVALUATE:\")\n","                    if len(parts) == 2:\n","                        emergency_chars = max(15000, 60000 - (attempt * 15000))\n","                        truncated_analysis = parts[1][:emergency_chars] + f\"\\n\\n[EMERGENCY TRUNCATION: {emergency_chars:,} chars]\"\n","                        current_prompt = parts[0] + \"GROUND TRUTH ANALYSIS TO EVALUATE:\" + truncated_analysis\n","                        continue\n","\n","            elif \"rate limit\" in error_str:\n","                # Shorter waits for batch processing\n","                wait_time = min(30 + (attempt * 10), 90)\n","                print(f\"    ‚è≥ {evaluation_id} - Rate limit, waiting {wait_time}s...\")\n","                logging.info(f\"    ‚è≥ {evaluation_id} - Rate limit, waiting {wait_time}s...\")\n","                time.sleep(wait_time)\n","                continue\n","\n","            # Progressive backoff\n","            if attempt < max_retries - 1:\n","                wait_time = min(5 + (attempt * 2), 20)\n","                time.sleep(wait_time)\n","            else:\n","                print(f\"    ‚ùå {evaluation_id} - Failed after {max_retries} attempts\")\n","                logging.error(f\"    ‚ùå {evaluation_id} - Failed after {max_retries} attempts\")\n","                return None\n","\n","    return None\n","\n","def process_single_evaluation(evaluation_task):\n","    \"\"\"Process a single evaluation task - designed for concurrent execution\"\"\"\n","    global success_counter, failure_counter, truncation_counter\n","\n","    crime_category, groundtruth_data, eval_number, total_evals = evaluation_task\n","    evaluation_id = f\"GroundTruth-{crime_category}\"\n","\n","    try:\n","        # Check if data will be truncated\n","        if isinstance(groundtruth_data, dict):\n","            data_size = sum(len(str(v)) for v in groundtruth_data.values())\n","        else:\n","            data_size = len(str(groundtruth_data))\n","\n","        if data_size > 200000:\n","            with evaluation_lock:\n","                truncation_counter += 1\n","\n","        # Create evaluation prompt\n","        prompt = create_groundtruth_evaluation_prompt(crime_category, groundtruth_data)\n","\n","        # Get evaluation from Claude\n","        response = evaluate_with_claude(prompt, evaluation_id)\n","\n","        if response:\n","            # Parse scores\n","            scores = parse_evaluation_response(response)\n","\n","            if scores:\n","                result = {\n","                    'crime_category': crime_category,\n","                    'total_score': scores['Total_Score'],\n","                    'detailed_scores': scores,\n","                    'success': True,\n","                    'response': response,\n","                    'evaluation_id': evaluation_id\n","                }\n","\n","                with evaluation_lock:\n","                    success_counter += 1\n","\n","                print(f\"    ‚úÖ {evaluation_id} - Score: {scores['Total_Score']}/100\")\n","                logging.info(f\"    ‚úÖ {evaluation_id} - Score: {scores['Total_Score']}/100\")\n","                return result\n","            else:\n","                print(f\"    ‚ùå {evaluation_id} - Failed to parse evaluation\")\n","                logging.error(f\"    ‚ùå {evaluation_id} - Failed to parse evaluation\")\n","                with evaluation_lock:\n","                    failure_counter += 1\n","                return {\n","                    'crime_category': crime_category,\n","                    'success': False,\n","                    'error': 'PARSE_ERROR',\n","                    'evaluation_id': evaluation_id\n","                }\n","        else:\n","            print(f\"    ‚ùå {evaluation_id} - No response\")\n","            logging.error(f\"    ‚ùå {evaluation_id} - No response\")\n","            with evaluation_lock:\n","                failure_counter += 1\n","            return {\n","                'crime_category': crime_category,\n","                'success': False,\n","                'error': 'NO_RESPONSE',\n","                'evaluation_id': evaluation_id\n","            }\n","\n","    except Exception as e:\n","        print(f\"    ‚ùå {evaluation_id} - Exception: {e}\")\n","        logging.error(f\"    ‚ùå {evaluation_id} - Exception: {e}\")\n","        with evaluation_lock:\n","            failure_counter += 1\n","        return {\n","            'crime_category': crime_category,\n","            'success': False,\n","            'error': f'EXCEPTION: {str(e)}',\n","            'evaluation_id': evaluation_id\n","        }\n","\n","def process_batch(batch_tasks, batch_number, total_batches):\n","    \"\"\"Process a batch of evaluations concurrently\"\"\"\n","    print(f\"\\nüöÄ BATCH {batch_number}/{total_batches} - Processing {len(batch_tasks)} ground truth evaluations\")\n","    logging.info(f\"üöÄ BATCH {batch_number}/{total_batches} - Processing {len(batch_tasks)} ground truth evaluations\")\n","\n","    batch_results = []\n","    batch_start_time = time.time()\n","\n","    # Process batch with ThreadPoolExecutor\n","    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n","        # Submit all tasks in the batch\n","        future_to_task = {\n","            executor.submit(process_single_evaluation, task): task\n","            for task in batch_tasks\n","        }\n","\n","        # Collect results as they complete\n","        for future in concurrent.futures.as_completed(future_to_task):\n","            task = future_to_task[future]\n","            try:\n","                result = future.result()\n","                batch_results.append(result)\n","            except Exception as exc:\n","                print(f\"    ‚ùå Task generated exception: {exc}\")\n","                logging.error(f\"    ‚ùå Task generated exception: {exc}\")\n","                # Create error result\n","                crime_category = task[0]\n","                batch_results.append({\n","                    'crime_category': crime_category,\n","                    'success': False,\n","                    'error': f'FUTURE_EXCEPTION: {str(exc)}',\n","                    'evaluation_id': f\"GroundTruth-{crime_category}\"\n","                })\n","\n","    batch_duration = time.time() - batch_start_time\n","    successful_in_batch = sum(1 for r in batch_results if r['success'])\n","\n","    print(f\"üìä BATCH {batch_number} COMPLETE - {successful_in_batch}/{len(batch_tasks)} successful ({batch_duration:.1f}s)\")\n","    logging.info(f\"üìä BATCH {batch_number} COMPLETE - {successful_in_batch}/{len(batch_tasks)} successful ({batch_duration:.1f}s)\")\n","\n","    return batch_results\n","\n","def create_evaluation_batches(groundtruth_data):\n","    \"\"\"Create optimized batches of evaluation tasks - MODIFIED FOR GROUND TRUTH\"\"\"\n","    print(\"üîß Creating ground truth evaluation batches...\")\n","    logging.info(\"üîß Creating ground truth evaluation batches...\")\n","\n","    all_tasks = []\n","\n","    # Create all evaluation tasks\n","    eval_number = 0\n","    for crime_category, groundtruth_analysis in groundtruth_data.items():\n","        eval_number += 1\n","        task = (crime_category, groundtruth_analysis, eval_number, 0)  # Will update total later\n","        all_tasks.append(task)\n","\n","    # Update total count in all tasks\n","    total_evals = len(all_tasks)\n","    all_tasks = [(t[0], t[1], t[2], total_evals) for t in all_tasks]\n","\n","    # Split into batches\n","    batches = []\n","    for i in range(0, len(all_tasks), BATCH_SIZE):\n","        batch = all_tasks[i:i + BATCH_SIZE]\n","        batches.append(batch)\n","\n","    print(f\"üì¶ Created {len(batches)} batches of {BATCH_SIZE} evaluations each\")\n","    print(f\"üìä Total ground truth evaluations: {total_evals}\")\n","    logging.info(f\"üì¶ Created {len(batches)} batches of {BATCH_SIZE} evaluations each, total: {total_evals}\")\n","\n","    return batches\n","\n","def retry_failed_batches(failed_results, groundtruth_data):\n","    \"\"\"Retry failed evaluations as separate batches\"\"\"\n","    if not failed_results:\n","        return []\n","\n","    print(f\"\\nüîÑ RETRYING {len(failed_results)} failed ground truth evaluations...\")\n","    logging.info(f\"üîÑ RETRYING {len(failed_results)} failed ground truth evaluations...\")\n","\n","    # Create retry tasks\n","    retry_tasks = []\n","    for result in failed_results:\n","        crime_category = result['crime_category']\n","\n","        if crime_category in groundtruth_data:\n","            analysis_data = groundtruth_data[crime_category]\n","            task = (crime_category, analysis_data, 0, len(failed_results))\n","            retry_tasks.append(task)\n","\n","    if not retry_tasks:\n","        return []\n","\n","    # Process retries in smaller batches\n","    retry_batch_size = max(1, BATCH_SIZE // 2)  # Smaller batches for retries\n","    retry_results = []\n","\n","    for i in range(0, len(retry_tasks), retry_batch_size):\n","        batch = retry_tasks[i:i + retry_batch_size]\n","        batch_number = (i // retry_batch_size) + 1\n","        total_retry_batches = (len(retry_tasks) + retry_batch_size - 1) // retry_batch_size\n","\n","        print(f\"üîÑ RETRY BATCH {batch_number}/{total_retry_batches}\")\n","        logging.info(f\"üîÑ RETRY BATCH {batch_number}/{total_retry_batches}\")\n","\n","        batch_results = process_batch(batch, f\"RETRY-{batch_number}\", total_retry_batches)\n","        retry_results.extend(batch_results)\n","\n","        # Longer delay between retry batches\n","        if i + retry_batch_size < len(retry_tasks):\n","            time.sleep(INTER_BATCH_DELAY * 2)\n","\n","    return retry_results\n","\n","def parse_evaluation_response(response_text):\n","    \"\"\"Parse Claude's evaluation response to extract scores\"\"\"\n","\n","    if not response_text:\n","        return None\n","\n","    scores = {}\n","    total_score = 0\n","    justification = \"\"\n","\n","    try:\n","        # Extract individual scores - UPDATED FOR GROUND TRUTH CRITERIA\n","        score_patterns = [\n","            r\"1\\.\\s*Crime Classification and Intent Detection:\\s*(\\d+)\",\n","            r\"2\\.\\s*Temporal Forensic Reconstruction:\\s*(\\d+)\",\n","            r\"3\\.\\s*Subject Identification and Behavioral Analysis:\\s*(\\d+)\",\n","            r\"4\\.\\s*Physical Evidence Documentation:\\s*(\\d+)\",\n","            r\"5\\.\\s*Violence Assessment and Weapon Analysis:\\s*(\\d+)\",\n","            r\"6\\.\\s*Criminal Network and Coordination Analysis:\\s*(\\d+)\",\n","            r\"7\\.\\s*Modus Operandi Documentation:\\s*(\\d+)\",\n","            r\"8\\.\\s*Scene Analysis and Environmental Context:\\s*(\\d+)\",\n","            r\"9\\.\\s*Escape Route and Exit Strategy Analysis:\\s*(\\d+)\",\n","            r\"10\\.\\s*Reference Quality and Training Utility:\\s*(\\d+)\"  # Updated criterion\n","        ]\n","\n","        criteria_names = [\n","            \"Crime_Classification\",\n","            \"Temporal_Reconstruction\",\n","            \"Subject_Identification\",\n","            \"Physical_Evidence\",\n","            \"Violence_Assessment\",\n","            \"Criminal_Network\",\n","            \"Modus_Operandi\",\n","            \"Scene_Analysis\",\n","            \"Escape_Route\",\n","            \"Reference_Quality\"  # Updated name\n","        ]\n","\n","        for i, pattern in enumerate(score_patterns):\n","            match = re.search(pattern, response_text, re.IGNORECASE)\n","            if match:\n","                score = int(match.group(1))\n","                scores[criteria_names[i]] = score\n","                total_score += score\n","\n","        # Extract total score\n","        total_match = re.search(r\"TOTAL SCORE:\\s*(\\d+)\", response_text, re.IGNORECASE)\n","        if total_match:\n","            reported_total = int(total_match.group(1))\n","            # Use calculated total if reported total doesn't match\n","            if reported_total != total_score and total_score > 0:\n","                print(f\"  ‚ö†Ô∏è  Total score mismatch: calculated={total_score}, reported={reported_total}\")\n","                logging.warning(f\"  ‚ö†Ô∏è  Total score mismatch: calculated={total_score}, reported={reported_total}\")\n","\n","        # Extract justification\n","        just_match = re.search(r\"BRIEF JUSTIFICATION:\\s*(.+?)(?:\\n\\n|\\Z)\", response_text, re.DOTALL | re.IGNORECASE)\n","        if just_match:\n","            justification = just_match.group(1).strip()\n","\n","        scores['Total_Score'] = total_score\n","        scores['Justification'] = justification\n","\n","        return scores\n","\n","    except Exception as e:\n","        print(f\"  ‚ùå Error parsing response: {e}\")\n","        logging.error(f\"  ‚ùå Error parsing response: {e}\")\n","        return None\n","\n","def verify_complete_data_processing(groundtruth_data, results):\n","    \"\"\"Verify that ALL available ground truth data has been processed - NO DATA LEFT BEHIND\"\"\"\n","\n","    verification_report = {\n","        'total_data_files': 0,\n","        'processed_files': 0,\n","        'unprocessed_files': [],\n","        'completion_percentage': 0,\n","        'missing_evaluations': []\n","    }\n","\n","    print(\"\\nüîç VERIFYING COMPLETE GROUND TRUTH DATA PROCESSING...\")\n","    logging.info(\"üîç VERIFYING COMPLETE GROUND TRUTH DATA PROCESSING...\")\n","\n","    # Count all available data\n","    for category, data in groundtruth_data.items():\n","        verification_report['total_data_files'] += 1\n","\n","        # Check if this category was processed successfully\n","        result = results.get(category, \"NOT_ATTEMPTED\")\n","\n","        if isinstance(result, (int, float)):  # Successful evaluation (got a score)\n","            verification_report['processed_files'] += 1\n","        else:\n","            verification_report['unprocessed_files'].append(category)\n","            verification_report['missing_evaluations'].append({\n","                'category': category,\n","                'result': result,\n","                'reason': result if isinstance(result, str) else \"UNKNOWN\"\n","            })\n","\n","    # Calculate completion\n","    if verification_report['total_data_files'] > 0:\n","        verification_report['completion_percentage'] = (\n","            verification_report['processed_files'] / verification_report['total_data_files'] * 100\n","        )\n","\n","    # Log verification results\n","    print(f\"üìä GROUND TRUTH DATA PROCESSING VERIFICATION:\")\n","    print(f\"  Total ground truth files available: {verification_report['total_data_files']}\")\n","    print(f\"  Files successfully processed: {verification_report['processed_files']}\")\n","    print(f\"  Files unprocessed: {len(verification_report['unprocessed_files'])}\")\n","    print(f\"  Completion rate: {verification_report['completion_percentage']:.2f}%\")\n","\n","    logging.info(f\"üìä GROUND TRUTH DATA PROCESSING VERIFICATION:\")\n","    logging.info(f\"  Total ground truth files: {verification_report['total_data_files']}\")\n","    logging.info(f\"  Successfully processed: {verification_report['processed_files']}\")\n","    logging.info(f\"  Unprocessed: {len(verification_report['unprocessed_files'])}\")\n","    logging.info(f\"  Completion rate: {verification_report['completion_percentage']:.2f}%\")\n","\n","    if verification_report['unprocessed_files']:\n","        print(f\"‚ö†Ô∏è  UNPROCESSED GROUND TRUTH FILES:\")\n","        logging.warning(f\"‚ö†Ô∏è  UNPROCESSED GROUND TRUTH FILES:\")\n","        for file_name in verification_report['unprocessed_files']:\n","            print(f\"    - {file_name}\")\n","            logging.warning(f\"    - {file_name}\")\n","    else:\n","        print(\"üéâ ALL GROUND TRUTH FILES HAVE BEEN PROCESSED!\")\n","        logging.info(\"üéâ ALL GROUND TRUTH FILES HAVE BEEN PROCESSED!\")\n","\n","    return verification_report\n","\n","def force_process_remaining_data(groundtruth_data, results, detailed_results):\n","    \"\"\"Force process any remaining unprocessed ground truth data files\"\"\"\n","\n","    print(\"\\nüîí FORCE PROCESSING REMAINING GROUND TRUTH DATA...\")\n","    logging.info(\"üîí FORCE PROCESSING REMAINING GROUND TRUTH DATA...\")\n","\n","    unprocessed_tasks = []\n","\n","    # Find unprocessed data\n","    for category, data in groundtruth_data.items():\n","        result = results.get(category, \"NOT_ATTEMPTED\")\n","\n","        if not isinstance(result, (int, float)):  # Not successfully processed\n","            unprocessed_tasks.append((category, data, 0, 0))\n","\n","    if not unprocessed_tasks:\n","        print(\"‚úÖ No unprocessed ground truth data found!\")\n","        logging.info(\"‚úÖ No unprocessed ground truth data found!\")\n","        return 0\n","\n","    print(f\"üîß Found {len(unprocessed_tasks)} unprocessed ground truth files\")\n","    logging.info(f\"üîß Found {len(unprocessed_tasks)} unprocessed ground truth files\")\n","\n","    # Process in smaller batches with more conservative settings\n","    force_batch_size = 3  # Smaller batches for problematic data\n","    force_processed = 0\n","\n","    for i in range(0, len(unprocessed_tasks), force_batch_size):\n","        batch = unprocessed_tasks[i:i + force_batch_size]\n","        batch_number = (i // force_batch_size) + 1\n","        total_force_batches = (len(unprocessed_tasks) + force_batch_size - 1) // force_batch_size\n","\n","        print(f\"üîß FORCE BATCH {batch_number}/{total_force_batches} - Processing {len(batch)} files\")\n","        logging.info(f\"üîß FORCE BATCH {batch_number}/{total_force_batches} - Processing {len(batch)} files\")\n","\n","        # Process with more conservative settings\n","        for task in batch:\n","            category, data, _, _ = task\n","            evaluation_id = f\"FORCE-GroundTruth-{category}\"\n","\n","            # Use most conservative prompt settings\n","            prompt = create_groundtruth_evaluation_prompt(category, data)\n","\n","            # Try with emergency truncation if needed\n","            if len(prompt) > 100000:  # Very conservative limit\n","                lines = prompt.split('\\n')\n","                analysis_start = None\n","                for j, line in enumerate(lines):\n","                    if \"GROUND TRUTH ANALYSIS TO EVALUATE:\" in line:\n","                        analysis_start = j + 1\n","                        break\n","\n","                if analysis_start:\n","                    pre_analysis = '\\n'.join(lines[:analysis_start])\n","                    analysis_section = '\\n'.join(lines[analysis_start:])\n","\n","                    if len(analysis_section) > 30000:  # Emergency limit\n","                        analysis_section = analysis_section[:30000] + \"\\n\\n[FORCE PROCESSING TRUNCATION]\"\n","\n","                    prompt = pre_analysis + analysis_section\n","\n","            # Single-threaded processing for force mode\n","            response = evaluate_with_claude(prompt, evaluation_id, max_retries=15)\n","\n","            if response:\n","                scores = parse_evaluation_response(response)\n","\n","                if scores and scores.get('Total_Score', 0) > 0:\n","                    # Success\n","                    results[category] = scores['Total_Score']\n","\n","                    # Add to detailed results\n","                    detailed_entry = {\n","                        'Crime_Category': category,\n","                        'Total_Score': scores['Total_Score'],\n","                        'Justification': scores.get('Justification', 'Force processed'),\n","                        'Raw_Response': response,\n","                        'Batch_Number': 'FORCE',\n","                        'Processing_Type': 'FORCE_COMPLETE'\n","                    }\n","\n","                    for criterion, score in scores.items():\n","                        if criterion not in ['Total_Score', 'Justification']:\n","                            detailed_entry[criterion] = score\n","\n","                    detailed_results.append(detailed_entry)\n","                    force_processed += 1\n","\n","                    print(f\"    ‚úÖ FORCE SUCCESS: {evaluation_id} - Score: {scores['Total_Score']}/100\")\n","                    logging.info(f\"    ‚úÖ FORCE SUCCESS: {evaluation_id} - Score: {scores['Total_Score']}/100\")\n","\n","                    # Update global counters\n","                    global success_counter\n","                    with evaluation_lock:\n","                        success_counter += 1\n","                else:\n","                    results[category] = \"FORCE_PARSE_FAILED\"\n","                    print(f\"    ‚ùå FORCE PARSE FAILED: {evaluation_id}\")\n","                    logging.error(f\"    ‚ùå FORCE PARSE FAILED: {evaluation_id}\")\n","            else:\n","                results[category] = \"FORCE_API_FAILED\"\n","                print(f\"    ‚ùå FORCE API FAILED: {evaluation_id}\")\n","                logging.error(f\"    ‚ùå FORCE API FAILED: {evaluation_id}\")\n","\n","            # Longer delay for force processing\n","            time.sleep(5)\n","\n","        # Longer delay between force batches\n","        if i + force_batch_size < len(unprocessed_tasks):\n","            time.sleep(20)\n","\n","    print(f\"\\nüìä FORCE PROCESSING SUMMARY:\")\n","    print(f\"  Unprocessed files found: {len(unprocessed_tasks)}\")\n","    print(f\"  Successfully force processed: {force_processed}\")\n","    print(f\"  Still failed after force processing: {len(unprocessed_tasks) - force_processed}\")\n","\n","    logging.info(f\"üìä FORCE PROCESSING SUMMARY:\")\n","    logging.info(f\"  Unprocessed files: {len(unprocessed_tasks)}\")\n","    logging.info(f\"  Force processed: {force_processed}\")\n","    logging.info(f\"  Still failed: {len(unprocessed_tasks) - force_processed}\")\n","\n","    return force_processed\n","\n","def save_batch_checkpoint(results, detailed_results, batch_number, total_batches, output_dir):\n","    \"\"\"Save checkpoint after each batch\"\"\"\n","    checkpoint_dir = os.path.join(output_dir, \"checkpoints\")\n","    os.makedirs(checkpoint_dir, exist_ok=True)\n","\n","    checkpoint_data = {\n","        'results': results,\n","        'detailed_results': detailed_results,\n","        'batch_progress': {\n","            'completed_batches': batch_number,\n","            'total_batches': total_batches,\n","            'completion_percentage': (batch_number / total_batches * 100) if total_batches > 0 else 0\n","        },\n","        'stats': {\n","            'successful_evaluations': success_counter,\n","            'failed_evaluations': failure_counter,\n","            'truncated_files': truncation_counter\n","        },\n","        'timestamp': datetime.now().isoformat()\n","    }\n","\n","    checkpoint_file = os.path.join(checkpoint_dir, \"groundtruth_checkpoint.json\")\n","    try:\n","        with open(checkpoint_file, 'w', encoding='utf-8') as f:\n","            json.dump(checkpoint_data, f, indent=2, ensure_ascii=False)\n","        print(f\"    üíæ Checkpoint saved ({batch_number}/{total_batches} batches)\")\n","        logging.info(f\"    üíæ Checkpoint saved ({batch_number}/{total_batches} batches)\")\n","    except Exception as e:\n","        print(f\"    ‚ö†Ô∏è  Checkpoint save failed: {e}\")\n","        logging.error(f\"    ‚ö†Ô∏è  Checkpoint save failed: {e}\")\n","\n","def run_groundtruth_evaluation():\n","    \"\"\"Main ground truth evaluation function - MODIFIED FOR GROUND TRUTH PROCESSING\"\"\"\n","\n","    print(\"üöÄ Starting Ground Truth Forensic Analysis Performance Evaluation\")\n","    print(\"üîí GUARANTEED 100% GROUND TRUTH DATA PROCESSING - ALL FILES WILL BE EVALUATED\")\n","    print(f\"üìÅ Source Directory: {SOURCE_DIR}\")\n","    print(f\"üíæ Output Directory: {OUTPUT_DIR}\")\n","    print(f\"üì¶ Batch Size: {BATCH_SIZE}\")\n","    print(f\"üßµ Max Workers: {MAX_WORKERS}\")\n","    print(f\"üéØ Strategy: Concurrent batch processing of ground truth with verification & force completion\")\n","\n","    # Setup\n","    os.makedirs(OUTPUT_DIR, exist_ok=True)\n","    log_file = setup_logging(OUTPUT_DIR)\n","\n","    # Check API key\n","    if not ANTHROPIC_API_KEY or not client:\n","        print(\"‚ùå Claude API key not loaded. Please check the API key file.\")\n","        logging.error(\"‚ùå Claude API key not loaded. Please check the API key file.\")\n","        return\n","\n","    # Load all ground truth data\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"üìñ LOADING GROUND TRUTH DATA\")\n","    print(\"=\"*60)\n","\n","    groundtruth_data, data_stats = load_groundtruth_files(SOURCE_DIR)\n","\n","    if not groundtruth_data:\n","        print(\"‚ùå No ground truth data found. Exiting.\")\n","        logging.error(\"‚ùå No ground truth data found. Exiting.\")\n","        return\n","\n","    # Create evaluation batches\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"üì¶ CREATING GROUND TRUTH EVALUATION BATCHES\")\n","    print(\"=\"*60)\n","\n","    batches = create_evaluation_batches(groundtruth_data)\n","\n","    if not batches:\n","        print(\"‚ùå No evaluation batches created. Exiting.\")\n","        logging.error(\"‚ùå No evaluation batches created. Exiting.\")\n","        return\n","\n","    # Initialize results\n","    results = {}\n","    detailed_results = []\n","    all_batch_results = []\n","\n","    # Process all batches\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"üîç PROCESSING GROUND TRUTH EVALUATION BATCHES\")\n","    print(\"=\"*60)\n","\n","    total_batches = len(batches)\n","\n","    for batch_idx, batch in enumerate(batches, 1):\n","        print(f\"\\nüìä Overall Progress: {batch_idx}/{total_batches} batches\")\n","        logging.info(f\"üìä Overall Progress: {batch_idx}/{total_batches} batches\")\n","\n","        # Process batch\n","        batch_results = process_batch(batch, batch_idx, total_batches)\n","        all_batch_results.extend(batch_results)\n","\n","        # Update results structure\n","        for result in batch_results:\n","            crime_category = result['crime_category']\n","\n","            if result['success']:\n","                results[crime_category] = result['total_score']\n","\n","                # Add to detailed results\n","                detailed_entry = {\n","                    'Crime_Category': crime_category,\n","                    'Total_Score': result['total_score'],\n","                    'Justification': result['detailed_scores'].get('Justification', ''),\n","                    'Raw_Response': result['response'],\n","                    'Batch_Number': batch_idx,\n","                    'Processing_Type': 'BATCH'\n","                }\n","\n","                # Add individual criterion scores\n","                for criterion, score in result['detailed_scores'].items():\n","                    if criterion not in ['Total_Score', 'Justification']:\n","                        detailed_entry[criterion] = score\n","\n","                detailed_results.append(detailed_entry)\n","            else:\n","                results[crime_category] = result['error']\n","\n","        # Save checkpoint after each batch\n","        save_batch_checkpoint(results, detailed_results, batch_idx, total_batches, OUTPUT_DIR)\n","\n","        # Inter-batch delay (except for last batch)\n","        if batch_idx < total_batches:\n","            print(f\"‚è≥ Inter-batch delay: {INTER_BATCH_DELAY}s\")\n","            logging.info(f\"‚è≥ Inter-batch delay: {INTER_BATCH_DELAY}s\")\n","            time.sleep(INTER_BATCH_DELAY)\n","\n","    # Retry failed evaluations\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"üîÑ RETRY PHASE - FAILED GROUND TRUTH EVALUATIONS\")\n","    print(\"=\"*60)\n","\n","    failed_results = [r for r in all_batch_results if not r['success']]\n","\n","    if failed_results:\n","        print(f\"üîÑ Found {len(failed_results)} failed ground truth evaluations to retry\")\n","        logging.info(f\"üîÑ Found {len(failed_results)} failed ground truth evaluations to retry\")\n","\n","        retry_results = retry_failed_batches(failed_results, groundtruth_data)\n","\n","        # Update results with retry successes\n","        for result in retry_results:\n","            if result['success']:\n","                crime_category = result['crime_category']\n","\n","                results[crime_category] = result['total_score']\n","\n","                # Add to detailed results\n","                detailed_entry = {\n","                    'Crime_Category': crime_category,\n","                    'Total_Score': result['total_score'],\n","                    'Justification': result['detailed_scores'].get('Justification', ''),\n","                    'Raw_Response': result['response'],\n","                    'Batch_Number': 'RETRY',\n","                    'Processing_Type': 'RETRY'\n","                }\n","\n","                for criterion, score in result['detailed_scores'].items():\n","                    if criterion not in ['Total_Score', 'Justification']:\n","                        detailed_entry[criterion] = score\n","\n","                detailed_results.append(detailed_entry)\n","\n","                # Update counters\n","                with evaluation_lock:\n","                    global success_counter, failure_counter\n","                    success_counter += 1\n","                    failure_counter = max(0, failure_counter - 1)\n","    else:\n","        print(\"üéâ No failed ground truth evaluations to retry!\")\n","        logging.info(\"üéâ No failed ground truth evaluations to retry!\")\n","\n","    # VERIFY COMPLETE DATA PROCESSING\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"üîç VERIFICATION: ENSURING ALL GROUND TRUTH DATA PROCESSED\")\n","    print(\"=\"*60)\n","\n","    verification_report = verify_complete_data_processing(groundtruth_data, results)\n","\n","    # FORCE PROCESS REMAINING DATA if needed\n","    if verification_report['completion_percentage'] < 100:\n","        print(\"\\n\" + \"=\"*60)\n","        print(\"üîí FORCE PROCESSING - NO GROUND TRUTH DATA LEFT BEHIND\")\n","        print(\"=\"*60)\n","\n","        force_processed = force_process_remaining_data(groundtruth_data, results, detailed_results)\n","\n","        # Re-verify after force processing\n","        final_verification = verify_complete_data_processing(groundtruth_data, results)\n","        print(f\"\\nüéØ FINAL VERIFICATION: {final_verification['completion_percentage']:.2f}% complete\")\n","        logging.info(f\"üéØ FINAL VERIFICATION: {final_verification['completion_percentage']:.2f}% complete\")\n","    else:\n","        print(\"üéâ VERIFICATION PASSED: 100% GROUND TRUTH DATA PROCESSING CONFIRMED!\")\n","        logging.info(\"üéâ VERIFICATION PASSED: 100% GROUND TRUTH DATA PROCESSING CONFIRMED!\")\n","        final_verification = verification_report\n","\n","    # Save final results\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"üíæ SAVING FINAL GROUND TRUTH RESULTS\")\n","    print(\"=\"*60)\n","\n","    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","\n","    # Get all crime categories\n","    crime_categories = sorted(list(groundtruth_data.keys()))\n","\n","    # Ensure all categories have results\n","    for category in crime_categories:\n","        if category not in results:\n","            results[category] = \"NOT_PROCESSED\"\n","\n","    # Save summary as series (since it's just one \"technique\" - ground truth)\n","    summary_df = pd.DataFrame({\n","        'Crime_Category': crime_categories,\n","        'Ground_Truth_Score': [results.get(cat, \"NOT_PROCESSED\") for cat in crime_categories]\n","    })\n","    summary_file = os.path.join(OUTPUT_DIR, f\"groundtruth_summary_{timestamp}.csv\")\n","    summary_df.to_csv(summary_file, index=False)\n","    print(f\"‚úÖ Summary saved: {summary_file}\")\n","    logging.info(f\"‚úÖ Summary saved: {summary_file}\")\n","\n","    # Save detailed results\n","    if detailed_results:\n","        detailed_df = pd.DataFrame(detailed_results)\n","        detailed_file = os.path.join(OUTPUT_DIR, f\"groundtruth_detailed_{timestamp}.csv\")\n","        detailed_df.to_csv(detailed_file, index=False)\n","        print(f\"‚úÖ Detailed results saved: {detailed_file}\")\n","        logging.info(f\"‚úÖ Detailed results saved: {detailed_file}\")\n","\n","    # Save comprehensive statistics\n","    total_evaluations = len(crime_categories)\n","    actual_data_files = final_verification.get('total_data_files', len(groundtruth_data))\n","\n","    final_stats = {\n","        'execution_summary': {\n","            'total_ground_truth_files': total_evaluations,\n","            'actual_data_files': actual_data_files,\n","            'successful_evaluations': success_counter,\n","            'failed_evaluations': failure_counter,\n","            'truncated_files': truncation_counter,\n","            'success_rate': (success_counter / actual_data_files * 100) if actual_data_files > 0 else 0,\n","            'total_batches_processed': total_batches,\n","            'batch_size': BATCH_SIZE,\n","            'max_workers': MAX_WORKERS,\n","            'verification_completion_percentage': final_verification.get('completion_percentage', 0),\n","            'all_data_processed': final_verification.get('completion_percentage', 0) >= 99.0\n","        },\n","        'data_loading': data_stats,\n","        'verification_report': final_verification,\n","        'timestamp': timestamp,\n","        'evaluator': 'Claude-Sonnet-4-GroundTruth',\n","        'primary_model': 'claude-sonnet-4-20250514'\n","    }\n","\n","    # Save raw results as JSON\n","    raw_file = os.path.join(OUTPUT_DIR, f\"groundtruth_complete_{timestamp}.json\")\n","    with open(raw_file, 'w', encoding='utf-8') as f:\n","        json.dump({\n","            'summary': results,\n","            'detailed': detailed_results,\n","            'statistics': final_stats,\n","            'batch_results': all_batch_results\n","        }, f, indent=2, ensure_ascii=False)\n","    print(f\"‚úÖ Complete data saved: {raw_file}\")\n","    logging.info(f\"‚úÖ Complete data saved: {raw_file}\")\n","\n","    # Print comprehensive summary\n","    print(\"\\n\" + \"=\"*80)\n","    print(\"üìä GROUND TRUTH EVALUATION SUMMARY - 100% DATA VERIFICATION\")\n","    print(\"=\"*80)\n","    print(summary_df.to_string(index=False))\n","\n","    print(f\"\\n\" + \"=\"*60)\n","    print(\"üìà GROUND TRUTH PROCESSING STATISTICS - COMPLETE DATA COVERAGE\")\n","    print(\"=\"*60)\n","    print(f\"üì¶ Total batches processed: {total_batches}\")\n","    print(f\"üßµ Concurrent workers used: {MAX_WORKERS}\")\n","    print(f\"üìä Batch size: {BATCH_SIZE}\")\n","    print(f\"üìÅ Ground truth files found: {actual_data_files}\")\n","    print(f\"‚úÖ Successful evaluations: {success_counter}\")\n","    print(f\"‚ùå Failed evaluations: {failure_counter}\")\n","    print(f\"‚ö†Ô∏è  Truncated files: {truncation_counter}\")\n","    print(f\"üîç Verification completion: {final_verification.get('completion_percentage', 0):.2f}%\")\n","\n","    # DEFINITIVE DATA PROCESSING CONFIRMATION\n","    if final_stats['execution_summary'].get('all_data_processed', False):\n","        print(\"\\nüèÜ CONFIRMATION: ALL AVAILABLE GROUND TRUTH DATA HAS BEEN PROCESSED!\")\n","        print(\"üîí GUARANTEE FULFILLED: 100% of ground truth files have been evaluated\")\n","        logging.info(\"üèÜ CONFIRMATION: ALL AVAILABLE GROUND TRUTH DATA HAS BEEN PROCESSED!\")\n","    else:\n","        remaining_pct = 100 - final_verification.get('completion_percentage', 0)\n","        print(f\"\\n‚ö†Ô∏è  {remaining_pct:.2f}% of ground truth data could not be processed despite maximum efforts\")\n","        print(\"üìä This may be due to corrupted files or API limitations\")\n","        logging.warning(f\"‚ö†Ô∏è  {remaining_pct:.2f}% of ground truth data could not be processed\")\n","\n","    if actual_data_files > 0:\n","        success_rate = success_counter / actual_data_files * 100\n","        print(f\"üéØ Success rate: {success_rate:.1f}%\")\n","\n","        if success_rate >= 95:\n","            print(\"üèÜ EXCELLENT: 95%+ success rate achieved with ground truth evaluation!\")\n","        elif success_rate >= 90:\n","            print(\"ü•á VERY GOOD: 90%+ success rate achieved with ground truth evaluation!\")\n","        elif success_rate >= 80:\n","            print(\"ü•à GOOD: 80%+ success rate achieved with ground truth evaluation!\")\n","        else:\n","            print(f\"üìä COMPLETED: {success_rate:.1f}% success rate with ground truth evaluation\")\n","\n","    print(f\"\\nüéâ Ground Truth Evaluation Complete!\")\n","    print(f\"üìÅ Results saved in: {OUTPUT_DIR}\")\n","    print(f\"üìù Log file: {log_file}\")\n","\n","    # Clean up checkpoint if successful\n","    if success_counter >= actual_data_files * 0.9:  # 90% success rate\n","        try:\n","            checkpoint_file = os.path.join(OUTPUT_DIR, \"checkpoints\", \"groundtruth_checkpoint.json\")\n","            if os.path.exists(checkpoint_file):\n","                os.remove(checkpoint_file)\n","                print(\"üßπ Cleanup: Removed checkpoint file\")\n","                logging.info(\"üßπ Cleanup: Removed checkpoint file\")\n","        except:\n","            pass\n","\n","if __name__ == \"__main__\":\n","    try:\n","        run_groundtruth_evaluation()\n","    except KeyboardInterrupt:\n","        print(\"\\n‚è∏Ô∏è  Ground truth evaluation interrupted by user\")\n","        print(\"üíæ Progress has been saved - run again to resume\")\n","        logging.info(\"‚è∏Ô∏è  Ground truth evaluation interrupted by user\")\n","    except Exception as e:\n","        print(f\"\\nüí• Unexpected error: {e}\")\n","        print(\"üíæ Progress has been saved - run again to resume\")\n","        logging.error(f\"üí• Unexpected error: {e}\", exc_info=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jeHlcN1dAjdC","executionInfo":{"status":"ok","timestamp":1751311375217,"user_tz":240,"elapsed":44164,"user":{"displayName":"opeyemi adeniran","userId":"06352095503427961436"}},"outputId":"12cb266d-23b7-4218-91a2-f2d65e8b1d8d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Claude API key loaded successfully\n","‚úÖ Claude client initialized\n","üîç Testing API connection...\n","‚úÖ API connection successful\n","üöÄ Starting Ground Truth Forensic Analysis Performance Evaluation\n","üîí GUARANTEED 100% GROUND TRUTH DATA PROCESSING - ALL FILES WILL BE EVALUATED\n","üìÅ Source Directory: /content/drive/Shareddrives/DR KOFI RESEARCH/RESEARCH/PROMPTS/GroundTruth\n","üíæ Output Directory: /content/drive/Shareddrives/DR KOFI RESEARCH/RESEARCH/PROMPTS/GROUNDTRUTH-EVALUATION-RESULTS\n","üì¶ Batch Size: 6\n","üßµ Max Workers: 6\n","üéØ Strategy: Concurrent batch processing of ground truth with verification & force completion\n","\n","============================================================\n","üìñ LOADING GROUND TRUTH DATA\n","============================================================\n","üîç Scanning ground truth directory: /content/drive/Shareddrives/DR KOFI RESEARCH/RESEARCH/PROMPTS/GroundTruth\n","üìÅ Found 11 ground truth files\n","    ‚úÖ Loaded: Vandalism\n","    ‚úÖ Loaded: Fighting\n","    ‚úÖ Loaded: Assault\n","    ‚úÖ Loaded: Shoplifting\n","    ‚úÖ Loaded: Arson\n","    ‚úÖ Loaded: Explosion\n","    ‚úÖ Loaded: Robbery\n","    ‚úÖ Loaded: Abuse\n","    ‚úÖ Loaded: Stealing\n","    ‚úÖ Loaded: Shooting\n","    ‚úÖ Loaded: Burglary\n","\n","üìä Ground Truth Loading Summary:\n","  Total files found: 11\n","  Successfully loaded: 11\n","  Failed to load: 0\n","\n","============================================================\n","üì¶ CREATING GROUND TRUTH EVALUATION BATCHES\n","============================================================\n","üîß Creating ground truth evaluation batches...\n","üì¶ Created 2 batches of 6 evaluations each\n","üìä Total ground truth evaluations: 11\n","\n","============================================================\n","üîç PROCESSING GROUND TRUTH EVALUATION BATCHES\n","============================================================\n","\n","üìä Overall Progress: 1/2 batches\n","\n","üöÄ BATCH 1/2 - Processing 6 ground truth evaluations\n","    ü§ñ GroundTruth-Vandalism - Attempt 1\n","    ü§ñ GroundTruth-Fighting - Attempt 1\n","    ü§ñ GroundTruth-Assault - Attempt 1\n","    ü§ñ GroundTruth-Shoplifting - Attempt 1\n","    ü§ñ GroundTruth-Arson - Attempt 1\n","    ü§ñ GroundTruth-Explosion - Attempt 1\n","    ‚úÖ GroundTruth-Arson - SUCCESS with claude-sonnet-4-20250514\n","    ‚úÖ GroundTruth-Arson - Score: 31/100\n","    ‚úÖ GroundTruth-Vandalism - SUCCESS with claude-sonnet-4-20250514\n","    ‚úÖ GroundTruth-Vandalism - Score: 31/100\n","    ‚úÖ GroundTruth-Fighting - SUCCESS with claude-sonnet-4-20250514\n","    ‚úÖ GroundTruth-Fighting - Score: 26/100\n","    ‚úÖ GroundTruth-Explosion - SUCCESS with claude-sonnet-4-20250514\n","    ‚úÖ GroundTruth-Explosion - Score: 18/100\n","    ‚úÖ GroundTruth-Shoplifting - SUCCESS with claude-sonnet-4-20250514\n","    ‚úÖ GroundTruth-Shoplifting - Score: 26/100\n","    ‚úÖ GroundTruth-Assault - SUCCESS with claude-sonnet-4-20250514\n","    ‚úÖ GroundTruth-Assault - Score: 35/100\n","üìä BATCH 1 COMPLETE - 6/6 successful (15.7s)\n","    üíæ Checkpoint saved (1/2 batches)\n","‚è≥ Inter-batch delay: 10s\n","\n","üìä Overall Progress: 2/2 batches\n","\n","üöÄ BATCH 2/2 - Processing 5 ground truth evaluations\n","    ü§ñ GroundTruth-Robbery - Attempt 1\n","    ü§ñ GroundTruth-Abuse - Attempt 1\n","    ü§ñ GroundTruth-Stealing - Attempt 1\n","    ü§ñ GroundTruth-Shooting - Attempt 1\n","    ü§ñ GroundTruth-Burglary - Attempt 1\n","    ‚úÖ GroundTruth-Stealing - SUCCESS with claude-sonnet-4-20250514\n","    ‚úÖ GroundTruth-Stealing - Score: 37/100\n","    ‚úÖ GroundTruth-Shooting - SUCCESS with claude-sonnet-4-20250514\n","    ‚úÖ GroundTruth-Shooting - Score: 23/100\n","    ‚úÖ GroundTruth-Burglary - SUCCESS with claude-sonnet-4-20250514\n","    ‚úÖ GroundTruth-Burglary - Score: 26/100\n","    ‚úÖ GroundTruth-Robbery - SUCCESS with claude-sonnet-4-20250514\n","    ‚úÖ GroundTruth-Robbery - Score: 39/100\n","    ‚úÖ GroundTruth-Abuse - SUCCESS with claude-sonnet-4-20250514\n","    ‚úÖ GroundTruth-Abuse - Score: 30/100\n","üìä BATCH 2 COMPLETE - 5/5 successful (11.7s)\n","    üíæ Checkpoint saved (2/2 batches)\n","\n","============================================================\n","üîÑ RETRY PHASE - FAILED GROUND TRUTH EVALUATIONS\n","============================================================\n","üéâ No failed ground truth evaluations to retry!\n","\n","============================================================\n","üîç VERIFICATION: ENSURING ALL GROUND TRUTH DATA PROCESSED\n","============================================================\n","\n","üîç VERIFYING COMPLETE GROUND TRUTH DATA PROCESSING...\n","üìä GROUND TRUTH DATA PROCESSING VERIFICATION:\n","  Total ground truth files available: 11\n","  Files successfully processed: 11\n","  Files unprocessed: 0\n","  Completion rate: 100.00%\n","üéâ ALL GROUND TRUTH FILES HAVE BEEN PROCESSED!\n","üéâ VERIFICATION PASSED: 100% GROUND TRUTH DATA PROCESSING CONFIRMED!\n","\n","============================================================\n","üíæ SAVING FINAL GROUND TRUTH RESULTS\n","============================================================\n","‚úÖ Summary saved: /content/drive/Shareddrives/DR KOFI RESEARCH/RESEARCH/PROMPTS/GROUNDTRUTH-EVALUATION-RESULTS/groundtruth_summary_20250630_192255.csv\n","‚úÖ Detailed results saved: /content/drive/Shareddrives/DR KOFI RESEARCH/RESEARCH/PROMPTS/GROUNDTRUTH-EVALUATION-RESULTS/groundtruth_detailed_20250630_192255.csv\n","‚úÖ Complete data saved: /content/drive/Shareddrives/DR KOFI RESEARCH/RESEARCH/PROMPTS/GROUNDTRUTH-EVALUATION-RESULTS/groundtruth_complete_20250630_192255.json\n","\n","================================================================================\n","üìä GROUND TRUTH EVALUATION SUMMARY - 100% DATA VERIFICATION\n","================================================================================\n","Crime_Category  Ground_Truth_Score\n","         Abuse                  30\n","         Arson                  31\n","       Assault                  35\n","      Burglary                  26\n","     Explosion                  18\n","      Fighting                  26\n","       Robbery                  39\n","      Shooting                  23\n","   Shoplifting                  26\n","      Stealing                  37\n","     Vandalism                  31\n","\n","============================================================\n","üìà GROUND TRUTH PROCESSING STATISTICS - COMPLETE DATA COVERAGE\n","============================================================\n","üì¶ Total batches processed: 2\n","üßµ Concurrent workers used: 6\n","üìä Batch size: 6\n","üìÅ Ground truth files found: 11\n","‚úÖ Successful evaluations: 11\n","‚ùå Failed evaluations: 0\n","‚ö†Ô∏è  Truncated files: 0\n","üîç Verification completion: 100.00%\n","\n","üèÜ CONFIRMATION: ALL AVAILABLE GROUND TRUTH DATA HAS BEEN PROCESSED!\n","üîí GUARANTEE FULFILLED: 100% of ground truth files have been evaluated\n","üéØ Success rate: 100.0%\n","üèÜ EXCELLENT: 95%+ success rate achieved with ground truth evaluation!\n","\n","üéâ Ground Truth Evaluation Complete!\n","üìÅ Results saved in: /content/drive/Shareddrives/DR KOFI RESEARCH/RESEARCH/PROMPTS/GROUNDTRUTH-EVALUATION-RESULTS\n","üìù Log file: /content/drive/Shareddrives/DR KOFI RESEARCH/RESEARCH/PROMPTS/GROUNDTRUTH-EVALUATION-RESULTS/logs/groundtruth_evaluation_20250630_192213.log\n","üßπ Cleanup: Removed checkpoint file\n"]}]},{"cell_type":"markdown","source":["/content/drive/Shareddrives/DR KOFI RESEARCH/RESEARCH/PROMPTS/RESULTS/CLAUDE-METRICS"],"metadata":{"id":"V4_NUlyb6VyR"}}]}